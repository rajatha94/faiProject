{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajatha94/faiProject/blob/master/vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO-oceZCT78R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Lambda, Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plot\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiXHanEzUMVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reparameterization_trick(args):\n",
        "    #Instead of sampling from Z directly, the samples are taken from epsilon which is a unit normal distribution.\n",
        "    mean, std_dev = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(mean)[0],latent_dim), mean=0, stddev=1.0)\n",
        "    return mean + K.exp(0.5 * std_dev) * epsilon\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnpM1psQUO-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_current_epoch(decoder,samples,epoch):\n",
        "    digit_size = 28\n",
        "    count = 1\n",
        "\n",
        "    fig = plot.figure(figsize=(6, 6))\n",
        "    print(\"Epoch :: \",epoch)\n",
        "    for sample in samples:\n",
        "        image = decoder.predict(sample)\n",
        "        digit = image[0].reshape(digit_size, digit_size)\n",
        "        plot.subplot(6, 6, count)\n",
        "        plot.imshow(digit, cmap='gray')\n",
        "        plot.axis('off')\n",
        "        count += 1\n",
        "    plot.savefig(\"vae_epoch_\"+str(epoch)+\".png\")\n",
        "    plot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx3FQ-e2UR6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the MNIST dataset\n",
        "(train_images, train_labels),(test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images = np.reshape(train_images, [-1, 784])\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "# network parameters\n",
        "batch_size = 64\n",
        "latent_dim = 2\n",
        "epochs = 5000\n",
        "\n",
        "# encoder layers\n",
        "inputs = Input(shape=(784,))\n",
        "encoder_hidden_1 = Dense(512, activation='relu')(inputs)\n",
        "encoder_hidden_2 = Dense(256, activation='relu')(encoder_hidden_1)\n",
        "mean = Dense(latent_dim)(encoder_hidden_2)\n",
        "std_dev = Dense(latent_dim)(encoder_hidden_2)\n",
        "\n",
        "# reparameterization trick\n",
        "z = Lambda(reparameterization_trick)([mean, std_dev])\n",
        "\n",
        "# encoder\n",
        "encoder = Model(inputs, mean)\n",
        "\n",
        "# decoder layers\n",
        "latent_inputs = Input(shape=(latent_dim,))\n",
        "decoder_hidden_layer_1 = Dense(256, activation='relu')\n",
        "decoder_hidden_layer_2 = Dense(512, activation='relu')\n",
        "decoder_output_layer = Dense(784, activation='sigmoid')\n",
        "\n",
        "# decoder\n",
        "decoder_hidden_1 = decoder_hidden_layer_1(latent_inputs)\n",
        "decoder_hidden_2 = decoder_hidden_layer_2(decoder_hidden_1)\n",
        "decoder_output = decoder_output_layer(decoder_hidden_2)\n",
        "decoder = Model(latent_inputs, decoder_output)\n",
        "\n",
        "# VAE\n",
        "vae_decoder_hidden_1 = decoder_hidden_layer_1(z)\n",
        "vae_decoder_hidden_2 = decoder_hidden_layer_2(vae_decoder_hidden_1)\n",
        "vae_decoder_output = decoder_output_layer(vae_decoder_hidden_2)\n",
        "vae = Model(inputs, vae_decoder_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d117tkRBa2_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    #Calculate the loss of the VAE\n",
        "    reconstruction_loss = K.sum(tf.keras.losses.binary_crossentropy(inputs,vae_decoder_output))\n",
        "    kl_loss = -0.5 * K.sum(1 + std_dev - K.square(mean) - K.exp(std_dev), axis=-1)\n",
        "    vae.add_loss(K.mean(reconstruction_loss + kl_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMYiS8oNUVhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    vae.compile(optimizer='adam')\n",
        "\n",
        "    random_samples = []\n",
        "    # Random samples for reconstructing the image\n",
        "    for i in range(36):\n",
        "      random_samples.append(np.random.normal(0,1,size=[batch_size, latent_dim]))\n",
        "    \n",
        "    plot_current_epoch(decoder,random_samples,0)\n",
        "    lossDict = {}\n",
        "    # train the autoencoder\n",
        "    for epoch in range(epochs):\n",
        "      vae.fit(train_images,epochs=1,batch_size=batch_size)\n",
        "      #if (epoch+1) % 100 == 0:\n",
        "      plot_current_epoch(decoder,random_samples,epoch+1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}